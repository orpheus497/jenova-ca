# Main Configuration for The JENOVA Cognitive Architecture
# SAFE DEFAULTS for 4GB VRAM (GTX 1650 Ti and similar)
model:
  # Path to GGUF model file (optional - will auto-discover if not found)
  # System searches: /usr/local/share/models, then ./models
  model_path: '/usr/local/share/models/model.gguf'

  # CPU threads for inference (-1 = auto-detect, 0 = use all available)
  # Auto-detection uses physical cores only (excludes hyperthreads)
  # Manual override: set to specific number (e.g., 4, 8, 16)
  threads: -1

  # GPU layers to offload
  # SAFE DEFAULT for 4GB VRAM: 0 (CPU-only mode)
  # For testing GPU: try 8, 16, or 20 layers
  # For 8GB+ VRAM: try -1 (auto) or 32 (all layers)
  gpu_layers: 0

  # Lock model in RAM to prevent swapping
  # SAFE DEFAULT: false (allows system to manage memory)
  mlock: false

  # Batch size for prompt processing
  # SAFE DEFAULT for limited VRAM: 256
  n_batch: 256

  # Context window size (tokens the model can process at once)
  # SAFE DEFAULT for 4GB VRAM: 4096
  # Reduce to 2048 if experiencing crashes
  context_size: 4096

  # Maximum tokens to generate per response
  max_tokens: 512

  # Temperature for generation (0.0 = deterministic, 1.0 = very creative)
  temperature: 0.7

  # Top-p (nucleus) sampling
  top_p: 0.95

  # Embedding model for semantic search (sentence-transformers)
  # Always uses CPU to preserve GPU memory for main LLM
  embedding_model: 'all-MiniLM-L6-v2'

  # Timeout for LLM generation (seconds)
  timeout_seconds: 120

# Hardware detection and optimization settings
hardware:
  # Show detailed hardware detection info at startup
  show_details: false

  # Preferred compute device: 'auto', 'cuda', 'cpu'
  # SAFE DEFAULT: 'cpu' (most reliable, no CUDA issues)
  # For NVIDIA GPU: try 'cuda' after testing CPU mode works
  prefer_device: 'cpu'

  # Memory management strategy
  # SAFE DEFAULT: 'balanced' (allows swap, no mlock)
  memory_strategy: 'balanced'

  # Enable real-time health monitoring
  enable_health_monitor: true

memory:
  preload_memories: true
  episodic_db_path: "memory_db/episodic"
  semantic_db_path: "memory_db/semantic"
  procedural_db_path: "memory_db/procedural"
  reflection_interval: 3 # Reflect every 3 conversation turns

cortex:
  relationship_weights:
    last_updated: 0.0
    elaborates_on: 1.5
    conflicts_with: 2.0
    related_to: 1.0
    develops: 1.5
    summarizes: 1.2
  pruning:
    enabled: true
    prune_interval: 10 # Prune every 10 reflection cycles
    max_age_days: 30 # Prune nodes older than 30 days
    min_centrality: 0.1 # Prune nodes with centrality less than 0.1

scheduler:
  generate_insight_interval: 5
  generate_assumption_interval: 7
  proactively_verify_assumption_interval: 8
  reflect_interval: 10
  reorganize_insights_interval: 10
  process_documents_interval: 15

# Phase 5: Cognitive Engine Timeouts
cognitive_engine:
  # Timeout for LLM generation during normal operation (seconds)
  llm_timeout: 120
  # Timeout for plan generation specifically (seconds)
  planning_timeout: 60

# Phase 5: RAG System Caching
rag_system:
  # Enable response caching (improves performance for repeated queries)
  cache_enabled: true
  # Maximum number of cached responses (LRU eviction)
  cache_size: 100
  # Timeout for response generation (seconds)
  generation_timeout: 120

# Phase 5: Enhanced Memory Search
memory_search:
  semantic_n_results: 5
  episodic_n_results: 3
  procedural_n_results: 3
  insight_n_results: 5
  # Enable LLM-based re-ranking of search results (improves relevance but slower)
  # Disable for faster search at the cost of relevance
  rerank_enabled: true
  # Timeout for re-ranking operation (seconds)
  rerank_timeout: 15

tools:
  file_sandbox_path: "~/jenova_files"
  # Whitelist of safe commands for the execute_shell_command tool.
  # Only commands listed here are allowed to be executed.
  shell_command_whitelist:
    - ls
    - cat
    - grep
    - find
    - echo
    - date
    - whoami
    - pwd
    - uname

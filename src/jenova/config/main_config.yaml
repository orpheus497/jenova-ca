# Main Configuration for The JENOVA Cognitive Architecture - JCA v3.0
# Optimized for AMD Ryzen 7 5700U with Radeon Graphics
hardware:
  threads: 16 # Number of CPU threads. Set to -1 or 0 for auto-detection (uses all available cores)
  gpu_layers: -1 # GPU layers to offload. Set to -1 to offload ALL layers (maximum GPU usage). Set to 0 to disable GPU
  mlock: false # Set to false to allow the OS to swap the model to disk, freeing up RAM.
model:
  model_path: "" # IMPORTANT: Replace with the actual path to your GGUF model file
  embedding_model: 'all-MiniLM-L6-v2'
  context_size: 4096 # Will be dynamically overridden by model's max capacity if smaller
  max_tokens: 1500
  temperature: 0.4
  top_p: 0.9
memory:
  preload_memories: true
  episodic_db_path: "memory_db/episodic"
  semantic_db_path: "memory_db/semantic"
  procedural_db_path: "memory_db/procedural"
  reflection_interval: 3 # Reflect every 3 conversation turns

cortex:
  relationship_weights:
    last_updated: null
    elaborates_on: 1.5
    conflicts_with: 2.0
    related_to: 1.0
    develops: 1.5
    summarizes: 1.2
  pruning:
    enabled: true
    prune_interval: 10 # Prune every 10 reflection cycles
    max_age_days: 30 # Prune nodes older than 30 days
    min_centrality: 0.1 # Prune nodes with centrality less than 0.1
  clustering:
    algorithm: "louvain" # Options: louvain, leiden, hierarchical
    min_cluster_size: 3 # Minimum nodes required to form a cluster
  metrics:
    enabled: true # Enable comprehensive graph metrics calculation
    calculate_clustering_coefficient: true # Calculate local clustering coefficient
    calculate_modularity: true # Calculate graph modularity and communities
  reflection:
    iterative_deepening: true # Enable iterative reflection for complex patterns
    max_iterations: 2 # Maximum number of reflection iterations
  integration:
    enabled: true # Enable Cortex-Memory integration
    memory_search_integration: true # Integrate Cortex centrality into memory search
    centrality_weighting: true # Use Cortex centrality scores in memory ranking
    relationship_aware_retrieval: true # Expand context using Cortex relationships
    max_expansion: 3 # Maximum number of context items to add via relationships
    memory_to_cortex_feedback: true # Enable feedback from Memory to Cortex
    cortex_to_memory_feedback: true # Enable feedback from Cortex to Memory
  proactive_engine:
    min_history_length: 3 # Minimum conversation turns before analyzing patterns
    suggestion_cooldown_hours: 1 # Minimum hours between proactive suggestions
    category_rotation: true # Rotate suggestion categories for variety

scheduler:
  generate_insight_interval: 5
  generate_assumption_interval: 7
  proactively_verify_assumption_interval: 8
  reflect_interval: 10
  reorganize_insights_interval: 10
  process_documents_interval: 15

memory_search:
  semantic_n_results: 5
  episodic_n_results: 3
  procedural_n_results: 3
  insight_n_results: 5

comprehension:
  query_analysis:
    enabled: true
    intent_classification: true
    entity_extraction: true
    complexity_analysis: true
    # Phase C.2 enhancements
    topic_modeling: true      # Extract main topics with category classification
    entity_linking: true      # Link entities to Cortex nodes
    reformulation: true       # Generate alternative query phrasings
    confidence_scoring: true  # Add confidence scores to all classifications
  planning:
    multi_level: true
    reasoning_chains: true
    max_sub_goals: 5
  context_scoring:
    enabled: true
    weights:
      semantic_similarity: 0.4
      entity_overlap: 0.3
      keyword_match: 0.2
      query_type_match: 0.1

organizing:
  ranking:
    enabled: true  # Enable multi-factor ranking algorithm
    factors:
      relevance_weight: 0.4  # Weight for semantic similarity (vector distance)
      recency_weight: 0.2    # Weight for temporal recency (exponential decay)
      centrality_weight: 0.2 # Weight for Cortex graph centrality
      type_weight: 0.1       # Weight for memory type priority
      confidence_weight: 0.1 # Weight for confidence score (semantic memories)
  memory_prioritization:
    insight: 1.0     # Highest priority: learned insights
    semantic: 0.8    # High priority: semantic facts
    episodic: 0.6    # Medium priority: episodic memories
    procedural: 0.5  # Lower priority: procedural memories
  context_organization:
    enabled: true  # Enable context categorization and tier classification
    categorization: true  # Enable topic-based categorization
    tier_classification: true  # Enable relevance tier classification (high/medium/low)

finetuning:
  training_file: "finetune_train.jsonl"
  lora_output_file: "models/lora-jenova-adapter.bin"
  finetuned_model_output: "models/jenova-finetuned.gguf"

tools:
  file_sandbox_path: "~/jenova_files"

performance:
  caching:
    enabled: true  # Enable caching for performance optimization
    centrality_cache_size: 500  # Maximum cached centrality scores
    centrality_cache_ttl: 300.0  # Time-to-live in seconds (5 minutes)
    node_search_cache_size: 1000  # Maximum cached node search results
    node_search_cache_ttl: 600.0  # Time-to-live in seconds (10 minutes)
    recency_cache_size: 500  # Maximum cached recency scores
    recency_cache_ttl: 180.0  # Time-to-live in seconds (3 minutes)
    similarity_cache_size: 2000  # Maximum cached similarity scores
    similarity_cache_ttl: 600.0  # Time-to-live in seconds (10 minutes)
    cleanup_interval: 300.0  # Cleanup expired entries every 5 minutes
  monitoring:
    enabled: true  # Enable performance monitoring
    log_interval: 3600.0  # Log performance summary every hour

logging:
  debug_enabled: false  # Enable DEBUG level logging to file (verbose, for development)
  log_file_name: "jenova.log"  # Name of the log file
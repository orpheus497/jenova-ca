# Main Configuration for The JENOVA Cognitive Architecture
# SAFE DEFAULTS for 4GB VRAM (GTX 1650 Ti and similar)
model:
  # Path to GGUF model file (optional - will auto-discover if not found)
  # System searches: /usr/local/share/models, then ./models
  model_path: '/usr/local/share/models/model.gguf'

  # CPU threads for inference (-1 = auto-detect, 0 = use all available)
  # Auto-detection uses physical cores only (excludes hyperthreads)
  # Manual override: set to specific number (e.g., 4, 8, 16)
  threads: -1

  # GPU layers to offload
  # Options: -1 (all layers), 0 (CPU only), 1-32 (specific count), 'auto' (detect)
  # RECOMMENDED: Use 'auto' for automatic detection based on available VRAM
  # Previous default was 20 for 4GB VRAM systems (GTX 1650 Ti)
  # Auto-detection provides optimal settings for all hardware tiers
  gpu_layers: auto

  # Lock model in RAM to prevent swapping
  # SAFE DEFAULT: false (allows system to manage memory)
  mlock: false

  # Batch size for prompt processing
  # SAFE DEFAULT for limited VRAM: 256
  n_batch: 256

  # Context window size (tokens the model can process at once)
  # SAFE DEFAULT for 4GB VRAM: 4096
  # Reduce to 2048 if experiencing crashes
  context_size: 4096

  # Maximum tokens to generate per response
  max_tokens: 512

  # Temperature for generation (0.0 = deterministic, 1.0 = very creative)
  temperature: 0.7

  # Top-p (nucleus) sampling
  top_p: 0.95

  # Embedding model for semantic search (sentence-transformers)
  # Always uses CPU to preserve GPU memory for main LLM
  embedding_model: 'all-MiniLM-L6-v2'

  # Timeout for LLM generation (seconds)
  # INCREASED for 4GB VRAM GPUs: at ~12 tok/s, 512 tokens takes ~43s
  timeout_seconds: 240

# Hardware detection and optimization settings
hardware:
  # Show detailed hardware detection info at startup
  show_details: false

  # Preferred compute device: 'auto', 'cuda', 'cpu'
  # PERFORMANCE MODE: Using CUDA for GPU acceleration
  # GTX 1650 Ti detected with 4GB VRAM
  prefer_device: 'cuda'

  # Memory management strategy
  # SAFE DEFAULT: 'balanced' (allows swap, no mlock)
  memory_strategy: 'balanced'

  # Enable real-time health monitoring
  enable_health_monitor: true

  # PyTorch GPU Access (NEW in v5.1.2)
  # Enable GPU access for PyTorch (used by sentence-transformers for embeddings)
  # Trade-off: Shares VRAM with main LLM, but provides 5-10x faster embeddings
  # RECOMMENDED: Keep false on 4GB VRAM systems (preserves VRAM for main LLM)
  # RECOMMENDED: Set true on 6GB+ VRAM systems (significant embedding speedup)
  # Default: false (all VRAM reserved for main LLM via llama-cpp-python)
  pytorch_gpu_enabled: false

memory:
  preload_memories: true
  episodic_db_path: "memory_db/episodic"
  semantic_db_path: "memory_db/semantic"
  procedural_db_path: "memory_db/procedural"
  reflection_interval: 3 # Reflect every 3 conversation turns

cortex:
  relationship_weights:
    last_updated: 0.0
    elaborates_on: 1.5
    conflicts_with: 2.0
    related_to: 1.0
    develops: 1.5
    summarizes: 1.2
  pruning:
    enabled: true
    prune_interval: 10 # Prune every 10 reflection cycles
    max_age_days: 30 # Prune nodes older than 30 days
    min_centrality: 0.1 # Prune nodes with centrality less than 0.1

scheduler:
  generate_insight_interval: 5
  generate_assumption_interval: 7
  proactively_verify_assumption_interval: 8
  reflect_interval: 10
  reorganize_insights_interval: 10
  process_documents_interval: 15

# Phase 5: Cognitive Engine Timeouts
# INCREASED for 4GB VRAM GPUs (GTX 1650 Ti achieving ~12 tokens/second)
# These values accommodate the full cognitive cycle with GPU acceleration
cognitive_engine:
  # Timeout for LLM generation during normal operation (seconds)
  # 512 tokens @ 12 tok/s = ~43s, adding 200% margin = 240s
  llm_timeout: 240
  # Timeout for plan generation specifically (seconds)
  # Planning typically generates ~100 tokens @ 12 tok/s = ~8s, with margin = 120s
  planning_timeout: 120

# Phase 5: RAG System Caching
rag_system:
  # Enable response caching (improves performance for repeated queries)
  cache_enabled: true
  # Maximum number of cached responses (LRU eviction)
  cache_size: 100
  # Timeout for response generation (seconds)
  # INCREASED for 4GB VRAM GPUs: 240s to match llm_timeout
  generation_timeout: 240

# Phase 5: Enhanced Memory Search
memory_search:
  semantic_n_results: 5
  episodic_n_results: 3
  procedural_n_results: 3
  insight_n_results: 5
  # Enable LLM-based re-ranking of search results (improves relevance but slower)
  # DISABLED for 4GB VRAM GPUs to reduce cognitive cycle time
  # Can be enabled on systems with 6GB+ VRAM achieving 25+ tokens/second
  rerank_enabled: false
  # Timeout for re-ranking operation (seconds)
  rerank_timeout: 30

tools:
  file_sandbox_path: "~/jenova_files"
  # Whitelist of safe commands for the execute_shell_command tool.
  # Only commands listed here are allowed to be executed.
  shell_command_whitelist:
    - ls
    - cat
    - grep
    - find
    - echo
    - date
    - whoami
    - pwd
    - uname

# Phase 8: Distributed Computing & LAN Networking
# Phase 9: Network mode enabled by default with graceful fallback to local-only
network:
  # Enable distributed mode (requires zeroconf, grpcio)
  # Default: true - Automatically discovers peers on LAN
  # Falls back gracefully to local-only if network initialization fails
  enabled: true

  # Operating mode: 'auto', 'local_only', 'distributed'
  # - auto: Start distributed, fallback to local if no peers (recommended)
  # - local_only: Disable networking, run standalone
  # - distributed: Require peers, fail if none available
  # Note: Network failures are non-critical and won't crash JENOVA
  mode: 'auto'

  # Service discovery configuration (mDNS/Zeroconf)
  discovery:
    service_name: 'jenova-ai'  # Service name for discovery
    port: 50051  # gRPC port for RPC communication
    ttl: 60  # Service advertisement TTL in seconds

  # Security configuration
  security:
    enabled: true  # Enable SSL/TLS encryption
    cert_dir: '~/.jenova-ai/certs'  # Certificate storage directory
    require_auth: true  # Require JWT authentication

  # Resource sharing settings
  resource_sharing:
    share_llm: true  # Offer LLM inference to peers
    share_embeddings: true  # Offer embedding generation to peers
    share_memory: false  # Share memory searches (privacy: disabled by default)
    max_concurrent_requests: 5  # Maximum concurrent peer requests

  # Peer selection strategy
  peer_selection:
    # Strategy: 'local_first', 'load_balanced', 'fastest', 'parallel_voting', 'round_robin'
    strategy: 'load_balanced'
    timeout_ms: 5000  # RPC timeout in milliseconds

# Phase 24: Adaptive Context Window Management
context_window:
  # Maximum tokens allowed in context window
  max_tokens: 4096
  # Start compression at this percentage full (0.0-1.0)
  compression_threshold: 0.8
  # Drop items below this priority score (0.0-1.0)
  min_priority_score: 0.3
  # Weights for relevance calculation (must sum to 1.0)
  relevance_weights:
    semantic_similarity: 0.4  # 40% - keyword/semantic overlap with query
    recency: 0.3              # 30% - how recently item was added
    frequency: 0.2            # 20% - how often item is accessed
    user_priority: 0.1        # 10% - user-specified importance
  # Compression settings
  compression:
    strategy: hybrid  # extractive, abstractive, or hybrid
    target_ratio: 0.3  # Compress to 30% of original size

# Phase 25: Self-Optimization Engine
self_optimization:
  # Enable autonomous parameter optimization
  enabled: false  # Opt-in feature (requires performance tracking)
  # Performance database path (relative to user data directory)
  performance_db_path: "optimization/performance.db"
  # Auto-optimization settings
  auto_optimize:
    enabled: false  # Automatically optimize parameters periodically
    interval_runs: 50  # Run optimization every N task executions
    min_data_points: 20  # Minimum data points before optimization
  # Bayesian optimization settings
  optimization:
    max_iterations: 30  # Maximum optimization iterations per task type
    exploration_weight: 0.1  # Balance exploration vs exploitation (0.0-1.0)
    convergence_tolerance: 0.001  # Improvement threshold for convergence
  # Task classification
  task_classification:
    auto_detect: true  # Automatically detect task types
    confidence_threshold: 0.6  # Minimum confidence for classification
  # Parameter search space (will be bounded by model capabilities)
  parameter_space:
    temperature:
      min: 0.0
      max: 1.0
    top_p:
      min: 0.0
      max: 1.0
    max_tokens:
      min: 128
      max: 2048

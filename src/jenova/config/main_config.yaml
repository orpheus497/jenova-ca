# Main Configuration for The JENOVA Cognitive Architecture - JCA v3.2
model:
  # Path to GGUF model file (required)
  model_path: './models/model.gguf'
  
  # CPU threads for inference (set based on your CPU)
  # Recommended: number of physical cores (not logical/hyperthreads)
  threads: 8
  
  # GPU layers to offload (-1 = all layers, 0 = CPU only)
  # For NVIDIA GPUs with CUDA: set to -1 for full GPU acceleration
  # For CPU-only: set to 0
  gpu_layers: -1
  
  # Lock model in RAM to prevent swapping (recommended for performance)
  mlock: true
  
  # Batch size for prompt processing
  n_batch: 512
  
  # Context window size (will be auto-detected from model, but can override)
  # Typical values: 2048, 4096, 8192, 16384, 32768
  context_size: 2048
  
  # Maximum tokens to generate per response
  max_tokens: 512
  
  # Temperature for generation (0.0 = deterministic, 1.0 = very creative)
  temperature: 0.7
  
  # Top-p (nucleus) sampling
  top_p: 0.95
  
  # Embedding model for semantic search (sentence-transformers)
  embedding_model: 'all-MiniLM-L6-v2'
memory:
  preload_memories: true
  episodic_db_path: "memory_db/episodic"
  semantic_db_path: "memory_db/semantic"
  procedural_db_path: "memory_db/procedural"
  reflection_interval: 3 # Reflect every 3 conversation turns

cortex:
  relationship_weights:
    last_updated: null
    elaborates_on: 1.5
    conflicts_with: 2.0
    related_to: 1.0
    develops: 1.5
    summarizes: 1.2
  pruning:
    enabled: true
    prune_interval: 10 # Prune every 10 reflection cycles
    max_age_days: 30 # Prune nodes older than 30 days
    min_centrality: 0.1 # Prune nodes with centrality less than 0.1

scheduler:
  generate_insight_interval: 5
  generate_assumption_interval: 7
  proactively_verify_assumption_interval: 8
  reflect_interval: 10
  reorganize_insights_interval: 10
  process_documents_interval: 15

memory_search:
  semantic_n_results: 5
  episodic_n_results: 3
  procedural_n_results: 3
  insight_n_results: 5

# Fine-tuning (DEPRECATED as of v3.2.0 - see finetune/DEPRECATED.md)
# These settings are retained for backward compatibility but are not used
finetuning:
  training_file: "finetune_train.jsonl"
  lora_output_dir: "./output/lora"
  finetuned_model_output: "./output/models/jenova-finetuned"

tools:
  file_sandbox_path: "~/jenova_files"
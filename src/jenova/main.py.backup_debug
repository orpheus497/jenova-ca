# The JENOVA Cognitive Architecture
# Copyright (c) 2024, orpheus497. All rights reserved.
#
# The JENOVA Cognitive Architecture is licensed under the MIT License.
# A copy of the license can be found in the LICENSE file in the root directory of this source tree.

"""This module is the main entry point for the JENOVA Cognitive Architecture.
"""

import os

# CRITICAL GPU Memory Management - Step 1: Blind PyTorch to CUDA
# This MUST be set BEFORE importing PyTorch (via any module)
# This prevents PyTorch from initializing CUDA context and consuming ~561 MB GPU memory
# Strategy:
#   - PyTorch (embeddings): CPU only, cannot see GPU
#   - llama-cpp-python (main LLM): Full GPU access via direct CUDA bindings
# All NVIDIA VRAM must be available for the main LLM via llama-cpp-python
os.environ['CUDA_VISIBLE_DEVICES'] = ''  # Hide CUDA from PyTorch during initialization
os.environ['PYTORCH_ALLOC_CONF'] = 'max_split_size_mb:32'

# Fix tokenizers parallelism warning when spawning subprocesses (e.g., web_search)
# This must be set before importing any libraries that use tokenizers
os.environ['TOKENIZERS_PARALLELISM'] = 'false'

import getpass
import queue
import traceback

from jenova.assumptions.manager import AssumptionManager
from jenova.cognitive_engine.engine import CognitiveEngine
from jenova.cognitive_engine.memory_search import MemorySearch
from jenova.cognitive_engine.rag_system import RAGSystem
from jenova.config import load_configuration
from jenova.cortex.cortex import Cortex
from jenova.infrastructure import (
    ErrorHandler, ErrorSeverity,
    HealthMonitor, MetricsCollector,
    FileManager, DataValidator
)
from jenova.insights.manager import InsightManager
from jenova.llm import (
    ModelManager, ModelLoadError,
    EmbeddingManager, EmbeddingLoadError,
    LLMInterface
)
from jenova.memory.episodic import EpisodicMemory
from jenova.memory.procedural import ProceduralMemory
from jenova.memory.semantic import SemanticMemory
from jenova.tools import ToolHandler
from jenova.ui.logger import UILogger
from jenova.ui.terminal import TerminalUI
from jenova.utils.file_logger import FileLogger
from jenova.utils.telemetry_fix import apply_telemetry_patch

apply_telemetry_patch()


def main():
    """Main entry point for The JENOVA Cognitive Architecture."""
    username = getpass.getuser()
    user_data_root = os.path.join(os.path.expanduser(
        "~"), ".jenova-ai", "users", username)
    os.makedirs(user_data_root, exist_ok=True)

    # Setup logging first
    message_queue = queue.Queue()
    ui_logger = UILogger(message_queue=message_queue)
    file_logger = FileLogger(user_data_root=user_data_root)

    llm_interface = None
    model_manager = None
    embedding_manager = None
    error_handler = None
    health_monitor = None
    metrics = None
    
    try:
        # --- Configuration ---
        ui_logger.progress_message("Loading configuration", 10)
        try:
            config = load_configuration(ui_logger, file_logger)
            config['user_data_root'] = user_data_root
            ui_logger.success("Configuration loaded")
        except Exception as e:
            ui_logger.error(f"Configuration error: {e}")
            ui_logger.error("Fix your configuration and try again.")
            return 1
        
        # --- Initialize Infrastructure (Phase 2) ---
        ui_logger.progress_message("Initializing infrastructure", 20)
        error_handler = ErrorHandler(ui_logger, file_logger)
        health_monitor = HealthMonitor(ui_logger, file_logger)
        metrics = MetricsCollector(ui_logger, file_logger)
        file_manager = FileManager(ui_logger, file_logger)
        ui_logger.success("Infrastructure initialized")

        # Check system health at startup (Phase 6: Enhanced display)
        ui_logger.progress_message("Checking system health", 30)
        with metrics.measure('startup_health_check'):
            health = health_monitor.get_health_snapshot()
            health_data = {
                'status': health.status.value,
                'cpu_percent': health.cpu_percent,
                'memory_percent': health.memory_percent,
                'memory_available_gb': health.memory_available_gb,
                'gpu_available': health.gpu_memory_total_mb is not None
            }
            if health.gpu_memory_total_mb is not None:
                gpu_memory_percent = (health.gpu_memory_used_mb / health.gpu_memory_total_mb) * 100
                health_data['gpu'] = {
                    'memory_percent': gpu_memory_percent,
                    'memory_used_mb': health.gpu_memory_used_mb
                }

            # Display health status using Phase 6 methods
            ui_logger.health_status(health_data)

            if health.status.value != 'healthy':
                for warning in health.warnings:
                    ui_logger.warning(f"  {warning}")

            file_logger.log_info(
                f"Startup health: CPU {health.cpu_percent:.1f}%, "
                f"Memory {health.memory_percent:.1f}% "
                f"({health.memory_available_gb:.1f}GB free)"
            )

        insights_root = os.path.join(user_data_root, "insights")
        cortex_root = os.path.join(user_data_root, "cortex")

        ui_logger.info(">> Initializing Intelligence Matrix...")

        # --- Language Models (Phase 3: New LLM Layer) ---
        # CRITICAL: Load LLM FIRST to claim GPU memory before PyTorch initializes CUDA
        print("DEBUG: About to load AI model")
        import sys
        sys.stdout.flush()
        ui_logger.progress_message("Loading AI model", 40)
        try:
            print("DEBUG: About to measure model_load_llm")
            import sys
            sys.stdout.flush()
            with metrics.measure('model_load_llm', {'gpu_layers': config['model']['gpu_layers']}):
                # Use new ModelManager (Phase 3)
                print("DEBUG: Creating ModelManager")
                sys.stdout.flush()
                model_manager = ModelManager(config, file_logger, ui_logger)
                print("DEBUG: ModelManager created, loading model")
                sys.stdout.flush()
                llm = model_manager.load_model()
                print("DEBUG: Model loaded")
                sys.stdout.flush()

                if not llm:
                    raise RuntimeError(
                        "LLM model could not be loaded. Check model path and integrity.")

                print("DEBUG: Creating LLMInterface")
                import sys
                sys.stdout.flush()
                # Use new LLMInterface (Phase 3)
                llm_interface = LLMInterface(config, ui_logger, file_logger, llm)
                print("DEBUG: LLMInterface created")
                sys.stdout.flush()

            # Log model load stats (Phase 6: Enhanced startup info)
            print("DEBUG: Getting model stats")
            import sys
            sys.stdout.flush()
            stats = metrics.get_stats('model_load_llm')
            print(f"DEBUG: stats = {stats}")
            sys.stdout.flush()
            model_info = model_manager.get_model_info()
            print(f"DEBUG: model_info = {model_info}")
            sys.stdout.flush()
            model_details = (
                f"{model_info.get('model_name', 'unknown')} | "
                f"ctx={model_info.get('context_size', 0)} | "
                f"gpu_layers={model_info.get('gpu_layers', 0)}"
            )
            print("DEBUG: About to call ui_logger.startup_info")
            sys.stdout.flush()
            ui_logger.startup_info("LLM Model", stats.avg_time, model_details)
            print("DEBUG: ui_logger.startup_info complete")
            sys.stdout.flush()
            file_logger.log_info(
                f"LLM loaded in {stats.avg_time:.2f}s - "
                f"{model_info.get('model_name', 'unknown')} "
                f"(ctx={model_info.get('context_size', 0)}, "
                f"gpu_layers={model_info.get('gpu_layers', 0)})"
            )
            print("DEBUG: Model loading section complete")
            sys.stdout.flush()
        except (ModelLoadError, Exception) as e:
            error_handler.handle_error(e, "Model Loading", ErrorSeverity.CRITICAL)
            recommendation = error_handler.get_cuda_recommendation()
            if recommendation:
                ui_logger.system_message(recommendation)
            return 1

        # Load embedding model AFTER main LLM
        # Embedding model runs on CPU to preserve all GPU VRAM for main LLM
        print("DEBUG: About to load embedding model")
        import sys
        sys.stdout.flush()
        ui_logger.progress_message("Loading embedding model", 55)
        try:
            print("DEBUG: Entered embedding model try block")
            sys.stdout.flush()
            with metrics.measure('model_load_embedding'):
                # CRITICAL: Embedding model MUST use CPU only
                # PyTorch was initialized with CUDA_VISIBLE_DEVICES='' so it cannot access GPU
                # This preserves all 4096 MB of GPU VRAM for the main LLM

                print("DEBUG: Creating EmbeddingManager")
                import sys
                sys.stdout.flush()
                # Use new EmbeddingManager (Phase 3)
                embedding_manager = EmbeddingManager(config, file_logger, ui_logger)
                print("DEBUG: EmbeddingManager created, loading model")
                sys.stdout.flush()
                embedding_manager.load_model()
                print("DEBUG: Embedding model load_model() complete")
                sys.stdout.flush()
                embedding_model = embedding_manager.embedding_model
                print(f"DEBUG: embedding_model = {embedding_model}")
                sys.stdout.flush()

                if not embedding_model:
                    raise RuntimeError("Embedding model could not be loaded.")

            print("DEBUG: Getting embedding stats")
            sys.stdout.flush()
            stats = metrics.get_stats('model_load_embedding')
            print(f"DEBUG: embedding stats = {stats}")
            sys.stdout.flush()
            embed_info = embedding_manager.get_model_info()
            print(f"DEBUG: embed_info = {embed_info}")
            sys.stdout.flush()
            embed_details = (
                f"{embed_info.get('model_name', 'unknown')} | "
                f"device={embed_info.get('device', 'CPU').upper()} | "
                f"dim={embed_info.get('dimension', 0)}"
            )
            print("DEBUG: About to call ui_logger.startup_info for embedding")
            sys.stdout.flush()
            ui_logger.startup_info("Embedding Model", stats.avg_time, embed_details)
            print("DEBUG: Embedding ui_logger.startup_info complete")
            sys.stdout.flush()
            file_logger.log_info(
                f"Embedding model loaded: {embed_info.get('model_name', 'unknown')} "
                f"on device: {embed_info.get('device', 'CPU')} in {stats.avg_time:.2f}s "
                f"(dim={embed_info.get('dimension', 0)}) - GPU VRAM fully reserved for LLM"
            )
        except (EmbeddingLoadError, Exception) as e:
            print(f"DEBUG: Embedding model error: {e}")
            traceback.print_exc()
            import sys
            sys.stdout.flush()
            error_handler.handle_error(e, "Embedding Model Loading", ErrorSeverity.CRITICAL)
            return 1

        # --- Memory Systems ---
        print("DEBUG: About to initialize memory systems")
        import sys
        sys.stdout.flush()
        ui_logger.progress_message("Initializing memory systems", 70)
        try:
            print("DEBUG: Entered memory systems try block")
            sys.stdout.flush()
            with metrics.measure('memory_systems_init'):
                print("DEBUG: Setting up memory paths")
                import sys
                sys.stdout.flush()
                episodic_mem_path = os.path.join(
                    user_data_root, 'memory', 'episodic')
                procedural_mem_path = os.path.join(
                    user_data_root, 'memory', 'procedural')
                semantic_mem_path = os.path.join(
                    user_data_root, 'memory', 'semantic')

                print("DEBUG: Creating SemanticMemory")
                sys.stdout.flush()
                try:
                    semantic_memory = SemanticMemory(
                        config, ui_logger, file_logger, semantic_mem_path, llm_interface, embedding_model)
                    print("DEBUG: SemanticMemory created")
                    sys.stdout.flush()
                except Exception as e:
                    print(f"DEBUG: SemanticMemory error: {e}")
                    traceback.print_exc()
                    sys.stdout.flush()
                    raise
                episodic_memory = EpisodicMemory(
                    config, ui_logger, file_logger, episodic_mem_path, llm_interface, embedding_model)
                print("DEBUG: EpisodicMemory created")
                sys.stdout.flush()
                procedural_memory = ProceduralMemory(
                    config, ui_logger, file_logger, procedural_mem_path, llm_interface, embedding_model)
                print("DEBUG: ProceduralMemory created")
                sys.stdout.flush()

            print("DEBUG: Getting memory systems stats")
            sys.stdout.flush()
            stats = metrics.get_stats('memory_systems_init')
            print("DEBUG: Calling ui_logger.startup_info for memory systems")
            sys.stdout.flush()
            ui_logger.startup_info("Memory Systems", stats.avg_time, "Semantic, Episodic, Procedural")
            print("DEBUG: Memory systems initialization complete")
            sys.stdout.flush()
        except Exception as e:
            error_handler.handle_error(e, "Memory System Initialization", ErrorSeverity.CRITICAL)
            return 1

        # --- Cognitive Core ---
        ui_logger.progress_message("Initializing cognitive core", 85)
        try:
            with metrics.measure('cognitive_core_init'):
                cortex = Cortex(config, ui_logger, file_logger,
                                llm_interface, cortex_root)
                memory_search = MemorySearch(
                    semantic_memory, episodic_memory, procedural_memory, config, file_logger)
                insight_manager = InsightManager(
                    config, ui_logger, file_logger, insights_root, llm_interface, cortex, memory_search)
                memory_search.insight_manager = insight_manager  # Circular dependency resolution
                assumption_manager = AssumptionManager(
                    config, ui_logger, file_logger, user_data_root, cortex, llm_interface)
                tool_handler = ToolHandler(config, ui_logger, file_logger)
                rag_system = RAGSystem(
                    llm_interface, memory_search, insight_manager, config)

            stats = metrics.get_stats('cognitive_core_init')
            core_details = "Cortex, RAG (Phase 5), Insights, Tools"
            ui_logger.startup_info("Cognitive Core", stats.avg_time, core_details)
        except Exception as e:
            error_handler.handle_error(e, "Cognitive Core Initialization", ErrorSeverity.CRITICAL)
            return 1

        # --- Final Engine and UI Setup ---
        ui_logger.progress_message("Finalizing cognitive engine", 95)

        # Log total startup time (Phase 6: Enhanced startup summary)
        startup_stats = {
            'llm_load': metrics.get_stats('model_load_llm').avg_time if metrics.get_stats('model_load_llm') else 0,
            'embedding_load': metrics.get_stats('model_load_embedding').avg_time if metrics.get_stats('model_load_embedding') else 0,
            'memory_init': metrics.get_stats('memory_systems_init').avg_time if metrics.get_stats('memory_systems_init') else 0,
            'cognitive_init': metrics.get_stats('cognitive_core_init').avg_time if metrics.get_stats('cognitive_core_init') else 0,
        }
        total_startup = sum(startup_stats.values())

        cognitive_engine = CognitiveEngine(llm_interface, memory_search, insight_manager,
                                           assumption_manager, config, ui_logger, file_logger, cortex, rag_system, tool_handler)

        # Store infrastructure in cognitive engine for use during operation (Phase 5)
        if hasattr(cognitive_engine, 'set_infrastructure'):
            cognitive_engine.set_infrastructure(
                health_monitor=health_monitor,
                metrics=metrics,
                error_handler=error_handler
            )

        # Display startup summary (Phase 6)
        ui_logger.progress_message("Ready", 100)
        ui_logger.info(">> Cognitive Engine: Online.")
        ui_logger.success(f"Startup complete in {total_startup:.2f}s")
        file_logger.log_info(f"Startup complete in {total_startup:.2f}s")
        file_logger.log_info(f"Startup breakdown: {startup_stats}")

        # Phase 6: Display RAG cache stats at startup
        if hasattr(rag_system, 'get_cache_stats'):
            cache_stats = rag_system.get_cache_stats()
            file_logger.log_info(f"RAG cache initialized: {cache_stats}")

        # Phase 6: Pass health_monitor and metrics to TerminalUI
        ui = TerminalUI(cognitive_engine, ui_logger, health_monitor=health_monitor, metrics=metrics)
        ui.run()
        
        # Log final metrics summary on clean shutdown
        if metrics:
            file_logger.log_info("=== Session Metrics Summary ===")
            metrics.log_summary(top_n=10)

    except KeyboardInterrupt:
        ui_logger.info("Shutdown requested by user")
        return 0
    except (RuntimeError, Exception) as e:
        error_message = f"Critical failure during startup: {e}"
        ui_logger.error(error_message)
        if file_logger:
            file_logger.log_error(error_message)
            file_logger.log_error(f"Traceback: {traceback.format_exc()}")
        return 1
    finally:
        # Ensure all resources are released (Phase 3: New cleanup)
        try:
            if llm_interface:
                llm_interface.close()
            if model_manager:
                model_manager.unload_model()
            if embedding_manager:
                embedding_manager.unload_model()
        except Exception as e:
            if file_logger:
                file_logger.log_error(f"Error during resource cleanup: {e}")

        shutdown_message = "JENOVA shutdown complete."
        if ui_logger:
            ui_logger.info(shutdown_message)
        if file_logger:
            file_logger.log_info(shutdown_message)


if __name__ == "__main__":
    print("DEBUG: About to call main()")
    import sys
    sys.stdout.flush()
    result = main()
    print(f"DEBUG: main() returned: {result}")
    sys.exit(result if result is not None else 0)
